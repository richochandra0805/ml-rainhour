{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)]\n",
        "(https://colab.research.google.com/github/richochandra0805/ml-rainhour/blob/main/Rainhour_Prediction.ipynb)"
      ],
      "metadata": {
        "id": "06X-uN52j56E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaZ-dCq5ZqVX",
        "outputId": "91744219-e94b-4a84-9a54-5da0b65108a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresi Rainhour dengan Auto Model Selection"
      ],
      "metadata": {
        "id": "1V2jjd5xZ9WU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ðŸ“¦ 1. INSTALL LIBRARY\n",
        "# ============================================\n",
        "!pip install scikit-learn xgboost openpyxl joblib -q\n",
        "\n",
        "# ============================================\n",
        "# ðŸ“¥ 2. LOAD DATA\n",
        "# ============================================\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/12822034-AcademicsLibrary/Kerja-Praktik/PT-Adaro-Indonesia-(2025)/4.Kerjaan/TOPIK1-ANNUALRAINHOURFORECAST/Data/Data_Rain_Manual1998-2025.xlsx\")\n",
        "df['Tanggal'] = pd.to_datetime(df['Tanggal'])\n",
        "df = df.dropna()\n",
        "\n",
        "# Feature Engineering\n",
        "df['dayofyear'] = df['Tanggal'].dt.dayofyear\n",
        "df['month'] = df['Tanggal'].dt.month\n",
        "\n",
        "X = df[['Rainfall', 'LostHour', 'Nino-3.4', 'dayofyear', 'month']]\n",
        "y = df['Rainhour']\n",
        "\n",
        "# ============================================\n",
        "# ðŸ”€ 3. SPLIT DATA\n",
        "# ============================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============================================\n",
        "# ðŸ¤– 4. TRAINING & AUTO-SELECTION MODEL\n",
        "# ============================================\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, r2_score\n",
        "\n",
        "models = {\n",
        "    \"rf\": Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', RandomForestRegressor(random_state=42))\n",
        "    ]),\n",
        "    \"xgb\": Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', XGBRegressor(objective='reg:squarederror', random_state=42))\n",
        "    ]),\n",
        "    \"mlp\": Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', MLPRegressor(random_state=42, max_iter=1000))\n",
        "    ])\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"rf\": {\n",
        "        \"model__n_estimators\": [100, 200],\n",
        "        \"model__max_depth\": [5, 10, None]\n",
        "    },\n",
        "    \"xgb\": {\n",
        "        \"model__n_estimators\": [100, 200],\n",
        "        \"model__max_depth\": [3, 5, 7]\n",
        "    },\n",
        "    \"mlp\": {\n",
        "        \"model__hidden_layer_sizes\": [(64,), (64, 32)],\n",
        "        \"model__activation\": ['relu', 'tanh']\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name in models:\n",
        "    print(f\"Training {name.upper()}...\")\n",
        "    search = GridSearchCV(models[name], params[name], cv=3, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
        "    search.fit(X_train, y_train)\n",
        "    y_pred = search.predict(X_test)\n",
        "    results[name] = {\n",
        "        \"best_score\": -search.best_score_,\n",
        "        \"test_mae\": mean_absolute_error(y_test, y_pred),\n",
        "        \"r2\": r2_score(y_test, y_pred),\n",
        "        \"best_params\": search.best_params_,\n",
        "        \"model\": search.best_estimator_\n",
        "    }\n",
        "\n",
        "# Tampilkan hasil semua model\n",
        "import pprint\n",
        "pprint.pprint({k: {x: results[k][x] for x in ['best_score', 'test_mae', 'r2']} for k in results})\n",
        "\n",
        "# Ambil model terbaik berdasarkan test_mae\n",
        "best_name = min(results, key=lambda x: results[x]['test_mae'])\n",
        "best_model = results[best_name]['model']\n",
        "print(f\"\\nModel terbaik: {best_name.upper()} dengan MAE {results[best_name]['test_mae']:.2f}\")\n",
        "\n",
        "# ============================================\n",
        "# ðŸ’¾ 5. SIMPAN MODEL\n",
        "# ============================================\n",
        "import joblib\n",
        "joblib.dump(best_model, f\"model_rainhour_best_{best_name}.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZVSX7VxiaEFk",
        "outputId": "b598f2cc-b9ed-4668-86fa-185ba5a4149f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training RF...\n",
            "Training XGB...\n",
            "Training MLP...\n",
            "{'mlp': {'best_score': np.float64(0.4914696613024194),\n",
            "         'r2': 0.9067183954724032,\n",
            "         'test_mae': 0.5050554195126451},\n",
            " 'rf': {'best_score': np.float64(0.4786446207437391),\n",
            "        'r2': 0.9027242255683187,\n",
            "        'test_mae': 0.48777123992268434},\n",
            " 'xgb': {'best_score': np.float64(0.49468497195640265),\n",
            "         'r2': 0.899333271145261,\n",
            "         'test_mae': 0.5003213937731403}}\n",
            "\n",
            "Model terbaik: RF dengan MAE 0.49\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model_rainhour_best_rf.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Klasifikasi Multi Output"
      ],
      "metadata": {
        "id": "hUPxImR3ao7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# ðŸ“¦ 1. INSTALL LIBRARY\n",
        "# ============================================\n",
        "!pip install scikit-learn xgboost joblib openpyxl -q\n",
        "\n",
        "# ============================================\n",
        "# ðŸ“¥ 2. LOAD DAN PERSIAPKAN DATA\n",
        "# ============================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/12822034-AcademicsLibrary/Kerja-Praktik/PT-Adaro-Indonesia-(2025)/4.Kerjaan/TOPIK1-ANNUALRAINHOURFORECAST/Data/Data_Rain_Manual1998-2025.xlsx\")\n",
        "df['Tanggal'] = pd.to_datetime(df['Tanggal'])\n",
        "df = df.dropna()\n",
        "\n",
        "# Buat fitur waktu\n",
        "df['dayofyear'] = df['Tanggal'].dt.dayofyear\n",
        "df['month'] = df['Tanggal'].dt.month\n",
        "\n",
        "# ============================================\n",
        "# ðŸ§ª 3. REKONSTRUKSI PROBABILITAS JAM HUJAN\n",
        "# ============================================\n",
        "# Strategi sederhana: asumsi hujan dimulai jam 14:00, durasi = Rainhour\n",
        "def rain_hours_binary(rainhour, start_hour=14):\n",
        "    hours = np.zeros(24)\n",
        "    dur = int(round(rainhour))\n",
        "    for i in range(dur):\n",
        "        h = (start_hour + i) % 24\n",
        "        hours[h] = 1\n",
        "    return hours\n",
        "\n",
        "rain_matrix = df['Rainhour'].apply(lambda rh: rain_hours_binary(rh)).to_list()\n",
        "rain_array = np.vstack(rain_matrix).astype(int)\n",
        "\n",
        "# Buat dataframe target multi-output\n",
        "df_rainhour_bin = pd.DataFrame(rain_array, columns=[f\"Rain_at_hour_{i}\" for i in range(24)])\n",
        "df_final = pd.concat([df, df_rainhour_bin], axis=1)\n",
        "\n",
        "# ============================================\n",
        "# ðŸŽ¯ 4. SPLIT FITUR DAN TARGET\n",
        "# ============================================\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "features = ['Rainfall', 'LostHour', 'Nino-3.4', 'dayofyear', 'month']\n",
        "targets = [f\"Rain_at_hour_{i}\" for i in range(24)]\n",
        "\n",
        "X = df_final[features]\n",
        "Y = df_final[targets]\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ============================================\n",
        "# ðŸ¤– 5. TRAINING MULTIOUTPUT CLASSIFIER\n",
        "# ============================================\n",
        "from sklearn.multioutput import MultiOutputClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, log_loss, roc_auc_score\n",
        "\n",
        "models = {\n",
        "    \"rf\": Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', MultiOutputClassifier(RandomForestClassifier(random_state=42)))\n",
        "    ]),\n",
        "    \"xgb\": Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', MultiOutputClassifier(XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)))\n",
        "    ]),\n",
        "    \"mlp\": Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('model', MultiOutputClassifier(MLPClassifier(max_iter=500, random_state=42)))\n",
        "    ])\n",
        "}\n",
        "\n",
        "params = {\n",
        "    \"rf\": {\n",
        "        \"model__estimator__n_estimators\": [100],\n",
        "        \"model__estimator__max_depth\": [5, 10]\n",
        "    },\n",
        "    \"xgb\": {\n",
        "        \"model__estimator__n_estimators\": [100],\n",
        "        \"model__estimator__max_depth\": [3, 5]\n",
        "    },\n",
        "    \"mlp\": {\n",
        "        \"model__estimator__hidden_layer_sizes\": [(64,), (64,32)],\n",
        "        \"model__estimator__activation\": ['relu']\n",
        "    }\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name in models:\n",
        "    print(f\"\\nTraining {name.upper()}...\")\n",
        "    search = GridSearchCV(models[name], params[name], cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    search.fit(X_train, Y_train)\n",
        "    Y_pred = search.predict(X_test)\n",
        "    results[name] = {\n",
        "        \"best_score\": search.best_score_,\n",
        "        \"test_accuracy\": np.mean(Y_pred == Y_test.values),\n",
        "        \"best_params\": search.best_params_,\n",
        "        \"model\": search.best_estimator_\n",
        "    }\n",
        "\n",
        "# ============================================\n",
        "# ðŸ“ˆ 6. EVALUASI HASIL\n",
        "# ============================================\n",
        "for name in results:\n",
        "    print(f\"\\n{name.upper()} Accuracy: {results[name]['test_accuracy']:.3f} | Best CV Score: {results[name]['best_score']:.3f}\")\n",
        "\n",
        "# Ambil model terbaik\n",
        "best_name = max(results, key=lambda x: results[x]['test_accuracy'])\n",
        "best_model = results[best_name]['model']\n",
        "print(f\"\\nâœ… Model terbaik: {best_name.upper()}\")\n",
        "\n",
        "# ============================================\n",
        "# ðŸ’¾ 7. SIMPAN MODEL\n",
        "# ============================================\n",
        "import joblib\n",
        "joblib.dump(best_model, f\"model_prob_rainhour_best_{best_name}.pkl\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9yAOHzRKaniL",
        "outputId": "ebf04d4e-8295-415c-cfd3-e3c9fd6d112b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training RF...\n",
            "\n",
            "Training XGB...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py:528: FitFailedWarning: \n",
            "2 fits failed out of a total of 6.\n",
            "The score on these train-test partitions for these parameters will be set to nan.\n",
            "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
            "\n",
            "Below are more details about the failures:\n",
            "--------------------------------------------------------------------------------\n",
            "1 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multioutput.py\", line 543, in fit\n",
            "    super().fit(X, Y, sample_weight=sample_weight, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multioutput.py\", line 274, in fit\n",
            "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1986, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
            "    estimator.fit(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 729, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\", line 1682, in fit\n",
            "    self._Booster = train(\n",
            "                    ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 729, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/training.py\", line 183, in train\n",
            "    bst.update(dtrain, iteration=i, fobj=obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 2246, in update\n",
            "    _check_call(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 310, in _check_call\n",
            "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
            "xgboost.core.XGBoostError: [02:22:10] /workspace/src/objective/./regression_loss.h:69: Check failed: base_score > 0.0f && base_score < 1.0f: base_score must be in (0,1) for logistic loss, got: 0\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x7fcd844a6ecc]\n",
            "  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xeda139) [0x7fcd850da139]\n",
            "  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x682723) [0x7fcd84882723]\n",
            "  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x682aec) [0x7fcd84882aec]\n",
            "  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x68d03b) [0x7fcd8488d03b]\n",
            "  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x7fcd843b6fa7]\n",
            "  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fcdb41dfe2e]\n",
            "  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fcdb41dc493]\n",
            "  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x7fcdb08e94d8]\n",
            "\n",
            "\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "1 fits failed with the following error:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_validation.py\", line 866, in _fit_and_score\n",
            "    estimator.fit(X_train, y_train, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py\", line 662, in fit\n",
            "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multioutput.py\", line 543, in fit\n",
            "    super().fit(X, Y, sample_weight=sample_weight, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/base.py\", line 1389, in wrapper\n",
            "    return fit_method(estimator, *args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multioutput.py\", line 274, in fit\n",
            "    self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 77, in __call__\n",
            "    return super().__call__(iterable_with_config)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1986, in __call__\n",
            "    return output if self.return_generator else list(output)\n",
            "                                                ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/joblib/parallel.py\", line 1914, in _get_sequential_output\n",
            "    res = func(*args, **kwargs)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/parallel.py\", line 139, in __call__\n",
            "    return self.function(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/multioutput.py\", line 63, in _fit_estimator\n",
            "    estimator.fit(X, y, **fit_params)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 729, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/sklearn.py\", line 1682, in fit\n",
            "    self._Booster = train(\n",
            "                    ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 729, in inner_f\n",
            "    return func(**kwargs)\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/training.py\", line 183, in train\n",
            "    bst.update(dtrain, iteration=i, fobj=obj)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 2246, in update\n",
            "    _check_call(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/xgboost/core.py\", line 310, in _check_call\n",
            "    raise XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
            "xgboost.core.XGBoostError: [02:22:13] /workspace/src/objective/./regression_loss.h:69: Check failed: base_score > 0.0f && base_score < 1.0f: base_score must be in (0,1) for logistic loss, got: 0\n",
            "Stack trace:\n",
            "  [bt] (0) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x2a6ecc) [0x7fcd844a6ecc]\n",
            "  [bt] (1) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0xeda139) [0x7fcd850da139]\n",
            "  [bt] (2) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x682723) [0x7fcd84882723]\n",
            "  [bt] (3) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x682aec) [0x7fcd84882aec]\n",
            "  [bt] (4) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(+0x68d03b) [0x7fcd8488d03b]\n",
            "  [bt] (5) /usr/local/lib/python3.11/dist-packages/xgboost/lib/libxgboost.so(XGBoosterUpdateOneIter+0x77) [0x7fcd843b6fa7]\n",
            "  [bt] (6) /lib/x86_64-linux-gnu/libffi.so.8(+0x7e2e) [0x7fcdb41dfe2e]\n",
            "  [bt] (7) /lib/x86_64-linux-gnu/libffi.so.8(+0x4493) [0x7fcdb41dc493]\n",
            "  [bt] (8) /usr/lib/python3.11/lib-dynload/_ctypes.cpython-311-x86_64-linux-gnu.so(+0xa4d8) [0x7fcdb08e94d8]\n",
            "\n",
            "\n",
            "\n",
            "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_search.py:1108: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [02:22:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/training.py:183: UserWarning: [02:22:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training MLP...\n",
            "\n",
            "RF Accuracy: 0.979 | Best CV Score: 0.669\n",
            "\n",
            "XGB Accuracy: 0.979 | Best CV Score: nan\n",
            "\n",
            "MLP Accuracy: 0.979 | Best CV Score: 0.659\n",
            "\n",
            "âœ… Model terbaik: RF\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['model_prob_rainhour_best_rf.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    }
  ]
}